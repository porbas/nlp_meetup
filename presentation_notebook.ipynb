{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYQ6Lh11LFkP"
   },
   "source": [
    "# Polish NLProc #6 - spaCy mówi po polsku, 6 listopada 2020\n",
    "\n",
    "\n",
    "Ryszard Tuora\n",
    "\n",
    "\n",
    "2 modele do języka polskiego w spaCy:\n",
    "\n",
    "IPI PAN - bardziej rozbudowany, wolniejszy, bardziej złożona instalacja - http://zil.ipipan.waw.pl/SpacyPL\n",
    "Więcej informacji: https://github.com/ipipan/spacy-pl\n",
    "\n",
    "model oficjalny - prostszy, znacznie szybszy, prosta instalacja - https://spacy.io/models/pl\n",
    "\n",
    "model IPI PAN dla języka polskiego\n",
    "\n",
    "składa się z:\n",
    "- taggera morfosyntaktycznego\n",
    "- lematyzatora\n",
    "- parsera zależnościowego\n",
    "- komponentu NER\n",
    "- flexera (komponentu do fleksji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RprQFxeM4zas"
   },
   "outputs": [],
   "source": [
    "# Przygotowanie środowiska, komendy linux\n",
    "# instalacja Morfeusza 2\n",
    "!wget -O - http://download.sgjp.pl/apt/sgjp.gpg.key|sudo apt-key add -\n",
    "!sudo apt-add-repository http://download.sgjp.pl/apt/ubuntu\n",
    "!sudo apt update\n",
    "!sudo apt install morfeusz2\n",
    "!sudo apt install python3-morfeusz2\n",
    "\n",
    "\n",
    "# instalacja spaCy\n",
    "\n",
    "!python3 -m pip install spacy\n",
    "\n",
    "# 1. instalacja modelu IPI PAN dla języka polskiego\n",
    "!wget \"http://zil.ipipan.waw.pl/SpacyPL?action=AttachFile&do=get&target=pl_spacy_model_morfeusz-0.1.3.tar.gz\"\n",
    "!mv 'SpacyPL?action=AttachFile&do=get&target=pl_spacy_model_morfeusz-0.1.3.tar.gz' pl_spacy_model_morfeusz-0.1.3.tar.gz\n",
    "!python3 -m pip install pl_spacy_model_morfeusz-0.1.3.tar.gz\n",
    "\n",
    "# linkowanie modelu do spaCy\n",
    "!python3 -m spacy link pl_spacy_model_morfeusz pl_spacy_model_morfeusz -f\n",
    "\n",
    "# 2. instalacja oficjalnego modelu spaCy\n",
    "!python3 -m spacy download pl_core_news_lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PtEk9Bt0N3u1"
   },
   "outputs": [],
   "source": [
    "### PYTHON 3\n",
    "# ładowanie modelu\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "nlp = spacy.load(\"pl_spacy_model_morfeusz\") # IPI PAN\n",
    "#nlp = spacy.load(\"pl_core_news_lg\") # OFICJALNY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zquQAX2HGVpg"
   },
   "source": [
    "# Część zero - Tokenizacja i reprezentacja tekstów\n",
    "\n",
    "Wejściem do modelu są łańcuchy tekstowe (stringi), na wyjściu dostajemy obiekt Doc reprezentujący struktury wykryte w tekście przez pełen potok (pipeline). Pierwszym krokiem który musi być wykonany aby przetworztć tekst, jest tokenizacja, czyli podział tekstu na tokeny/segmenty. Tokeny w większości przypadków odpowiadają słowom \"od spacji do spacji\". Ale warto zwrócić uwagę na kilka przypadków odstających od takiej prostej reguły.\n",
    "\n",
    "Wynikiem potoku spaCy jest obiekt Doc, który składa się z obiektów Token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrMbeDcfHIvu"
   },
   "outputs": [],
   "source": [
    "txt = \"Chciałby, żebym pojechał do miasta z zielono-żółto-białą flagą (np. Zielonej Góry).\"\n",
    "split = txt.split()\n",
    "doc = nlp(txt)\n",
    "print(\"    spaCy     vs.   .split()\\n\")\n",
    "for i in range(max([len(split), len(doc)])):\n",
    "    try:\n",
    "        tok1 = doc[i]\n",
    "    except IndexError:\n",
    "        tok1 = \"\"\n",
    "    try:\n",
    "        tok2 = split[i]\n",
    "    except IndexError:\n",
    "        tok2 = \"\"\n",
    "    print(\"{0:2}. {1:15} {2:15}\".format(i, tok1.orth_, tok2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy pozwala na zapisywanie przetworzonych dokumentów w formacie JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "doc_json = doc.to_json()\n",
    "json_string = json.dumps(doc_json, indent=2, ensure_ascii=False)\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4a5Izt4Tsae"
   },
   "source": [
    "# Część pierwsza - reprezentacje wektorowe\n",
    "\n",
    "Sercem modelu jest reprezentacja wektorowa słów, 500 tys. wektorów o długości 100 wyekstrahowanych z embeddingów word2vec KGR10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwvGXnaON7CB"
   },
   "source": [
    "### Bardzo popularny przykład \"arytmetyki słów\":\n",
    "znaczenie słów jest reprezentowane przez wektory, dla których mamy zdefiniowane operacje matematyczne. Możemy więc odjąć od znaczenia słowa \"królowa\", znaczenie słowa \"kobieta\", i dodać doń znaczenie słowa \"mężczyzna\", licząc iż wektor będący wynikiem takiego działania odpowiada słowu \"król\". W praktyce wektor taki najprawdopodobniej nie ma interpretacji, ale możemy znaleźć najbliższy wektor który ma jakąkolwiek interpretację przeszukując słownik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# od wersji 2.3 wektory są ładowane \"leniwie\"\n",
    "# musimy więc najpierw wymusić załadowanie wszystkich wektorów\n",
    "print(len(nlp.vocab))\n",
    "for s in nlp.vocab.vectors:\n",
    "    _ = nlp.vocab[s]\n",
    "print(len(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jb9ZXk4a8ety"
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    " \n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    " \n",
    "man = nlp.vocab['mężczyzna'].vector\n",
    "woman = nlp.vocab['kobieta'].vector\n",
    "queen = nlp.vocab['królowa'].vector\n",
    "king = nlp.vocab['król'].vector\n",
    " \n",
    "\n",
    "maybe_king = man - woman + queen\n",
    "computed_similarities = []\n",
    " \n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors\n",
    "    if not word.has_vector:\n",
    "        continue \n",
    "    similarity = cosine_similarity(maybe_king, word.vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    " \n",
    "sorted_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "print([w[0].text for w in sorted_similarities[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy konstruuje także wektory dla zdań, i dokumentów. Wektor dla dokumentu jest dostępny w atrybucie .vector obiektu Doc, i jest równy uśrednionemu wektorowi tokenów z tego dokumentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector = doc.vector\n",
    "mean_document_vector = sum([tok.vector for tok in doc])/len(doc)\n",
    "print(document_vector == mean_document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuPWYncPLyK4"
   },
   "source": [
    "# Część druga - Tagowanie morfosyntaktyczne\n",
    "korzystamy z tagsetu NKJP\n",
    "Nasz tagger to słownikowy tagger Morfeusz2 + dezambiguacja za pomocą neuronowego Toyggera (biLSTM)\n",
    "\n",
    "Każdy token t ma trzy interesujące nas atrybuty: \n",
    "- t.tag_ : klasa gramatyczna według polskiego tagsetu NKJP (http://nkjp.pl/poliqarp/help/ense2.html)\n",
    "- t.pos_ : klasa gramatyczna według międzynarodowego tagsetu UD (mapowana z NKJP)\n",
    "- t._.feats : customowy atrybut odpowiadający cechom morfosyntaktycznym (np. rodzajowi gramatycznemu, lub liczbie), poszczególne wartości cech są oddzielone dwukropkiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8MPXgjqU2LH"
   },
   "outputs": [],
   "source": [
    "txt = \"Nornica prowadzi zmierzchowo-nocny tryb życia, ale wychodzi również za dnia w poszukiwaniu pokarmu.\"\n",
    "doc = nlp(txt) # przetworzenie textu przez pipeline, na wyjściu dostajemy iterowalny obiekty klasy Doc, przechowujący tokeny\n",
    "\n",
    "print(\"{0:15} {1:8} {2:6} {3:15}\\n\".format(\".orth_\", \"NKJP\", \"UD POS\", \"._.feats\"))\n",
    "for t in doc:\n",
    "    print(\"{0:15} {1:8} {2:6} {3:15}\".format(t.orth_, t.tag_, t.pos_, t._.feats)) # wypisujemy interpretację morfosyntaktyczną każdego tokenu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzHlNwhrthsl"
   },
   "source": [
    "### Fleksja\n",
    "\n",
    "Flexer pozwala na odmianę pojedynczych tokenów, do pożądanej charakterystyki morfologicznej. Argumentami do flexera jest słowo do odmiany (string, lub lepiej otagowany token), i przedzielony dwukropkami string znaczników morfosyntaktycznych.\n",
    "\n",
    "Flexer umożliwia np. wypełnianie szablonów tekstów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4bvg3thmuzt"
   },
   "outputs": [],
   "source": [
    "filizanka = \"filiżanka\"\n",
    "flexer = nlp.get_pipe(\"flexer\")\n",
    "\n",
    "tmpl1 = \"Szukam szarej, ceramicznej {}.\".format(flexer.flex(filizanka, \"gen\"))\n",
    "tmpl2 = \"Marzę o szarej, ceramicznej {}.\".format(flexer.flex(filizanka, \"loc\"))\n",
    "tmpl3 = \"Szukam kompletu ceramicznych {}.\".format(flexer.flex(filizanka, \"gen:pl\"))\n",
    "print(tmpl1)\n",
    "print(tmpl2)\n",
    "print(tmpl3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flexer pozwala także na odmianę fraz wielowyrazowych (multi-word expressions - MWE).\n",
    "\n",
    "W tym celu możemy do niego podać string, lub token odpowiadający głowie frazy.\n",
    "Korzystamy z metody .flex_mwe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filizanka2 = \"biała, porcelanowa filiżanka z Chin\"\n",
    "\n",
    "tmpl1 = \"Szukam {}.\".format(flexer.flex_mwe(filizanka2, \"gen\"))\n",
    "tmpl2 = \"Marzę o {}.\".format(flexer.flex_mwe(filizanka2, \"loc\"))\n",
    "tmpl3 = \"Szukam kompletu {}.\".format(flexer.flex_mwe(filizanka2, \"gen:pl\"))\n",
    "\n",
    "print(tmpl1)\n",
    "print(tmpl2)\n",
    "print(tmpl3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noq6HpCgY4q2"
   },
   "source": [
    "# Część trzecia - Lematyzacja i własności leksykalne\n",
    "Nasz model umożliwia słownikową lematyzację przy pomocy Morfeusza, do dezambiguacji (tutaj np. rozróżnienia między \"Głos zabrały mamy dzieci.\"-> \"mama\" i \"My mamy samochód.\" -> \"mieć\" służy output taggera).\n",
    "\n",
    "Lematyzacja pozwala redukować redundancję informacyjną, i ułatwiać zadania takie jak streszczanie, i przeszukiwanie.\n",
    "\n",
    "Każdy z tokenów dodatkowo jest oznaczony ze względu na pewne własności leksykalne, np. :\n",
    "- t.lemma_ - lemat - forma reprezentatywna danego leksemu\n",
    "- t.is_stop - słowo należy do stoplisty (listy słów występujących najczęściej, a więc najmniej istotnych semantycznie)\n",
    "- t.is_oov - słowo znajduje się poza słownikiem, i.e. embeddingami wykorzystanymi w modelu\n",
    "- t.like_url - token ma strukturę url-a\n",
    "- t.like_num - token jest liczbą\n",
    "- t.is_alpha - token składa się tylko ze znaków alfabetycznych\n",
    "- t.rank - miejse w rankingu częstości słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFmVQkr2ZsKu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt = \"Zdenerwowany gen. Leese mówił przez telefon swym podwładnym walczącym pod Monte Cassino, że rozmawia z nimi ze schronu.\"\n",
    "doc = nlp(txt)\n",
    "print(\"{0:16} {1:16} {2:5} {3:5} {4:20}\\n\".format(\"forma\", \"lemat\", \"OOV\", \"STOP\", \"Częstość\"))\n",
    "for t in doc:\n",
    "    print(\"{0:16} {1:16} {2:5} {3:5} {4:20}\".format(t.orth_, t.lemma_, t.is_oov, t.is_stop, t.rank)) # orth_ to atrybut odpowiadający formie słowa występującej w tekście"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yev0e8lQmwMm"
   },
   "source": [
    "# Część czwarta - parsowanie zależnościowe\n",
    "\n",
    "spaCy zawiera parser zależnościowy oparty o metodologię transition-based dependency parsing. \n",
    "Interesują nas tu cztery atrybuty:\n",
    " - t.head - link do tokenu będącego nadrzędnikiem tokenu t\n",
    " - t.dep_ - etykieta opisująca rodzaj zależności\n",
    " - t.subtree - generator opisujący poddrzewo rozpięte przez token t\n",
    " - t.children - generator opisujący wszystkich bezpośrednich potomków tokenu t\n",
    " - t.ancestors - generator opisujący wszystkie przechodnie nadrzędniki tokenu t\n",
    "\n",
    "\n",
    "Opis systemu etykiet: https://universaldependencies.org/u/dep/all.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7i9joa5n3ZK"
   },
   "outputs": [],
   "source": [
    "txt = \"Pierwsza wzmianka o Gdańsku pochodzi ze spisanego po łacinie w 999 Żywotu świętego Wojciecha.\"\n",
    "doc = nlp(txt)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "table = []\n",
    "for tok in doc:\n",
    "    tok_dic = {\"form\": tok.orth_, \"label\": tok.dep_, \"head\": tok.head.orth_, \"subtree\": list(tok.subtree), \"ancestors\": list(tok.ancestors)}\n",
    "    table.append(tok_dic)\n",
    "df = pd.DataFrame(table)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6MI0U4Npeqn"
   },
   "source": [
    "### spaCy posiada wbudowaną wizualizację drzew zależnościowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFX-8z48peHV"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JS6zUzTKmN86"
   },
   "source": [
    "### Poniższa funkcja służy łatwej wizualizacji podstawowych własności tokenów z danego tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDcn4tWTLBwz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def table(doc):\n",
    "    table = []\n",
    "    for tok in doc:\n",
    "        tok_dic = {\"form\": tok.orth_, \"lemma\": tok.lemma_, \"tag\": \":\".join([tok.tag_, tok._.feats]), \"dep_label\": tok.dep_, \"dep_head\": tok.head.orth_}\n",
    "        table.append(tok_dic)\n",
    "    return pd.DataFrame(table)\n",
    "\n",
    "txt = \"Wiadomość jest symboliczna, ale oznacza też początek długotrwałego trendu.\\\n",
    " Dochód na mieszkańca z uwzględnieniem realnej mocy nabywczej walut narodowych \\\n",
    " wyniósł w 2019 r. w Rzeczpospolitej Polskiej 33 891 dolarów, nieco więcej, niż w Portugalii \\\n",
    " (33 665 dolarów). Jednak Fundusz przewiduje, że w tym roku portugalska gospodarka\\\n",
    "  będzie się rozwijać w tempie 1,6 proc. wobec 3,1 proc. w przypadku gospodarki \\\n",
    "  polskiej. Nożyce między oboma krajami będą się więc rozwierać.\"\n",
    "\n",
    "doc = nlp(txt)\n",
    "\n",
    "tab = table(doc)\n",
    "\n",
    "print(tab.to_string()) # prosty hack na wypisanie całej tabeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YG0hKgyWwc4N"
   },
   "source": [
    "### Parser zależnościowy pozwala także na dzielenie dokumentów na zdania w sposób bardziej inteligentny, niż posługując się regułami interpunkcji.\n",
    "Za zdanie uważamy nieprzerwaną sekwencję tokenów które są powiązane relacjami zależnościowymi.\n",
    "Zdania są zapisane w atrybucie doc.sents dokumentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fH4rg0X4wwIB"
   },
   "outputs": [],
   "source": [
    "for s in doc.sents:\n",
    "    print(s)\n",
    "\n",
    "displacy.render(doc, jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6d5JbrSsnhm"
   },
   "source": [
    "# Część czwarta - Rozpoznawanie jednostek nazewniczych (NER)\n",
    "#### Nasz model do spaCy wykorzystuje 6 rodzajów etykiet:\n",
    "- placeName - miejsca antropogeniczne, np. Dania, Londyn\n",
    "- geogName - naturalne miejsca geograficzne, np. Tatry, Kreta\n",
    "- persName - imiona i nazwiska osób, np. J. F. Kennedy, gen. Maczek\n",
    "- orgName - nazwy organizacji, np. NATO, Unia Europejska\n",
    "- date - daty, np. 22 marca 1999, druga połowa kwietnia\n",
    "- time - czas, np. 18:55, pięć po dwunastej\n",
    "\n",
    "#### Nie pozwala na wykrywanie zagnieżdżonych jednostek nazewniczych, np. [placeName: **aleja** [persName: **Piłsudskiego**]]\n",
    "\n",
    "Wykryte wzmianki są przechowywane w atrybucie doc.ents dokumentu, każda z tych wzmianek ma atrybut ent.label_, w którym przechowywana jest jej etykieta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Bkf6kOWt8GK"
   },
   "outputs": [],
   "source": [
    "print(doc, \"\\n\\n\")\n",
    "for e in doc.ents:\n",
    "    print(e, e.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zniWaxZxvYZz"
   },
   "source": [
    "### displaCy pozwala także na wizualizację jednostek nazewniczych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CJOleJmIvfGs"
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cknnUsl1uaKb"
   },
   "source": [
    "W obecnym modelu NER, uwzględnione są \"uniwersalne\" kategorie jednostek nazewniczych, jednak w zależności od zastosowania będziemy najprawdopodobniej potrzebowali innych kategorii - np. osobnej kategorii dla nazw aktów prawodawczych, lub kwot i walut.\n",
    "\n",
    "Bez problemu można zastąpić domyślny model własnym, wytrenowanym przez CLI spaCy na własnych oznakowanych danych. Więcej informacji tu: https://spacy.io/api/cli#train\n",
    "\n",
    "Jednostki które da się wykryć na podstawie reguł, można wykrywać poprzez EntityRuler: https://spacy.io/api/entityruler"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zquQAX2HGVpg",
    "PuPWYncPLyK4",
    "noq6HpCgY4q2",
    "yev0e8lQmwMm",
    "E6d5JbrSsnhm",
    "_4a5Izt4Tsae"
   ],
   "name": "Copy of Copy of Copy of Copy of Copy of Gdańsk.ipynb",
   "provenance": [
    {
     "file_id": "1MWiYw3ugMhpq38tBhkbWi1ZMLHU17IqZ",
     "timestamp": 1594737900299
    },
    {
     "file_id": "1lfYKkZyhW-hfQ6b0qsXxrVTjGquz8Qdn",
     "timestamp": 1594727658531
    },
    {
     "file_id": "1onFhNTAHRwnqjbCsYx1BdgoE76O5Ssix",
     "timestamp": 1594667047412
    },
    {
     "file_id": "1APwoUXjdVCZQVF6B1zMJUy5VZPQU1m43",
     "timestamp": 1594650744668
    },
    {
     "file_id": "https://github.com/ryszardtuora/webinar_resources/blob/master/Gda%C5%84sk.ipynb",
     "timestamp": 1594596301441
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
